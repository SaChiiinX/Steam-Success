import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import make_column_transformer, ColumnTransformer
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error, roc_auc_score, accuracy_score
from sklearn.linear_model import ElasticNetCV, LogisticRegression, LinearRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier, BaggingRegressor, BaggingClassifier
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA, TruncatedSVD
from matplotlib.ticker import MultipleLocator
from IPython.display import display
orig = pd.read_csv('../data/games_prelim.csv')
new = pd.read_csv('../data/games.csv')
new = new.drop_duplicates()
columns_to_keep = ['NAME', 'NEGATIVE REVIEWS','POSITIVE REVIEWS', 'TOTAL REVIEWS', 'ALL TIME PEAK', 'positive_review_rate',
'peak_ratio','log_negative_reviews', 'log_positive_reviews', 'log_total_reviews', 'log_all_time_peak']
merged = orig.merge(new[columns_to_keep], on='NAME', how='left')
merged = merged.drop_duplicates(subset='NAME', keep='first')
merged = merged.drop('Unnamed: 0', axis=1)
merged.to_csv('../data/new_success.csv')
data = pd.read_csv('../data/new_success.csv')
data = data.drop('Unnamed: 0', axis=1)
# drops first column
games = data
# turns publisher and developer into categorical columns
games["PUBLISHER"] = pd.Categorical(games["PUBLISHER"])
games["DEVELOPER"] = pd.Categorical(games["DEVELOPER"])
# splits data into successful and unsuccessful
succ = games[games["SUCCESS"] == 1].drop(["NAME","SUCCESS"], axis = 1)
fail = games[games["SUCCESS"] == 0].drop(["NAME","SUCCESS"], axis = 1)
games_kmeans = games.drop(["NAME","SUCCESS"], axis = 1)
# define categorical columns
categorical_columns = ["DEVELOPER", "PUBLISHER"]
# define numerical columns
numerical_columns = ["BASE PRICE", 'NEGATIVE REVIEWS', 'POSITIVE REVIEWS', "TOTAL REVIEWS", 'ALL TIME PEAK', 'positive_review_rate', 'peak_ratio', 'log_negative_reviews', 'log_positive_reviews', 'log_total_reviews', 'log_all_time_peak']
# initializes a preprocessor
preprocessor = make_column_transformer(
(OneHotEncoder(), categorical_columns),
(StandardScaler(), numerical_columns),
remainder='passthrough',
verbose_feature_names_out = False
)
#| message: false
# helper function for scree plot
def generate_inertia_plot(data, title, name):
# Preprocess data
categorical_columns = data.select_dtypes(include=['object']).columns
numerical_columns = data.select_dtypes(include=['int', 'float']).columns
# create preprocessor
preprocessor = ColumnTransformer(
transformers=[
('num', StandardScaler(), numerical_columns),
('cat', OneHotEncoder(), categorical_columns)
])
# fit data
X = preprocessor.fit_transform(data)
# find inertia
inertia = []
for k in range(1, 10):
kmeans = KMeans(n_clusters=k, n_init=20, random_state=13).fit(X)
inertia.append(kmeans.inertia_)
# create inertia df
inertia_df = pd.DataFrame({"k": range(1, 10), "Inertia": inertia})
# Plotting
fig, ax = plt.subplots()
sns.lineplot(data=inertia_df, x="k", y="Inertia", ax=ax)
ax.set(title=title, xlabel="Number of Clusters (k)", ylabel="Inertia")
ax.xaxis.set_major_locator(MultipleLocator(1))
plt.tight_layout()
plt.show()
# generate scree plot for successful games
generate_inertia_plot(succ, title="Scree Plot for Successful Games", succ_scree)
# generate scree plot for unsuccessful games
generate_inertia_plot(fail, title="Scree Plot for Unsuccessful Games", fail_scree)
# generate a KMeans pipeline with 3 clusters for successful games
succ_clusters = Pipeline([
("preprocessor", preprocessor),
("kmeans", KMeans(n_clusters=6, n_init=20, random_state=8))
]).fit(succ)
# add in cluster id for successful df
succ['cluster_id'] = succ_clusters["kmeans"].labels_
# generate a KMeans pipeline with 3 clusters for non-successful games
fail_clusters = Pipeline([
("preprocessor", preprocessor),
("kmeans", KMeans(n_clusters=4, n_init=20, random_state=8))
]).fit(fail)
# add in cluster id for non-successful df
fail['cluster_id'] = fail_clusters["kmeans"].labels_
#| fig-alt: This shows the size differences between the three clusters in successful games
fig, ax = plt.subplots()
sns.set(style='whitegrid', palette='colorblind')
sns.countplot(x = succ['cluster_id'], ax = ax)
ax.set(xlabel = 'Cluster ID', ylabel = 'Number of Observations', title='Sizes of Each Cluster Group For Successful Games')
plt.tight_layout()
plt.show()
plt.savefig("../figures/succ_cluster_size.jpg")
#| fig-alt: This shows the size differences between the three clusters in non-successful games
fig, ax = plt.subplots()
sns.set(style='whitegrid', palette='colorblind')
sns.countplot(x = fail['cluster_id'], ax = ax)
ax.set(xlabel = 'Cluster ID', ylabel = 'Number of Observations', title='Sizes of Each Cluster Group For Non-Successful Games')
plt.tight_layout()
plt.show()
plt.savefig("../figures/fail_cluster_size.jpg")
genre_columns = ['Action', 'Adventure', 'Casual', 'Early Access', 'Free to Play', 'Indie', 'Massively Multiplayer', 'Movie', 'RPG', 'Racing', 'Simulation', 'Sports', 'Strategy']
def add_dict(dataframe):
dictionary = {}
for genre in genre_columns:
dictionary[genre] = dataframe[genre].sum()
return dictionary
def print_dict(dataframe):
print("-------------------------------------------------------")
total_width = 52
for genre in dataframe:
name = genre + ':'
print(
f"{name.ljust(total_width//2)}{(str(dataframe[genre])).rjust(total_width//2)}")
print("-------------------------------------------------------")
#| message: false
# grab each clusters' subsection
succ_0 = succ[succ['cluster_id'] == 0]
cluster0_length = len(succ_0)
succ_1 = succ[succ['cluster_id'] == 1]
cluster1_length = len(succ_1)
succ_2 = succ[succ['cluster_id'] == 2]
cluster2_length = len(succ_2)
succ_3 = succ[succ['cluster_id'] == 3]
cluster0_length = len(succ_3)
succ_4 = succ[succ['cluster_id'] == 4]
cluster1_length = len(succ_4)
succ_5 = succ[succ['cluster_id'] == 5]
cluster2_length = len(succ_5)
print(f"The number of observations belonging to cluster 0 is {cluster0_length}, for cluster 1 is {cluster1_length}, and there are {cluster2_length} for cluster 2.\n")
# make dictionaries of each cluster + full dataframe
succ_dict = add_dict(succ)
succ_0_dict = add_dict(succ_0)
succ_1_dict = add_dict(succ_1)
succ_2_dict = add_dict(succ_2)
succ_3_dict = add_dict(succ_0)
succ_4_dict = add_dict(succ_1)
succ_5_dict = add_dict(succ_2)
# print results of each one
print('The result for the entire success dataframe in regards to genre is: ')
print_dict(succ_dict)
print()
print('The result for the cluster 0 success dataframe in regards to genre is: ')
print_dict(succ_0_dict)
print()
print('The result for the cluster 1 success dataframe in regards to genre is: ')
print_dict(succ_1_dict)
print()
print('The result for the cluster 2 success dataframe in regards to genre is: ')
print_dict(succ_2_dict)
print('The result for the cluster 3 success dataframe in regards to genre is: ')
print_dict(succ_3_dict)
print()
print('The result for the cluster 4 success dataframe in regards to genre is: ')
print_dict(succ_4_dict)
print()
print('The result for the cluster 5 success dataframe in regards to genre is: ')
print_dict(succ_5_dict)
# grab each clusters' subsection
fail_0 = fail[fail['cluster_id'] == 0]
cluster0_length = len(fail_0)
fail_1 = fail[fail['cluster_id'] == 1]
cluster1_length = len(fail_1)
fail_2 = fail[fail['cluster_id'] == 2]
cluster2_length = len(fail_2)
fail_3 = fail[fail['cluster_id'] == 3]
cluster2_length = len(fail_3)
print(f"The number of observations belonging to cluster 0 is {cluster0_length}, for cluster 1 is {cluster1_length}, and there are {cluster2_length} for cluster 2.\n")
# make dictionaries of each cluster + full dataframe
fail_dict = add_dict(fail)
fail_0_dict = add_dict(fail_0)
fail_1_dict = add_dict(fail_1)
fail_2_dict = add_dict(fail_2)
fail_3_dict = add_dict(fail_3)
# print results of each one
print('The result for the entire failure dataframe in regards to genre is: ')
print_dict(fail_dict)
print()
print('The result for the cluster 0 failure dataframe in regards to genre is: ')
print_dict(fail_0_dict)
print()
print('The result for the cluster 1 failure dataframe in regards to genre is: ')
print_dict(fail_1_dict)
print()
print('The result for the cluster 2 failure dataframe in regards to genre is: ')
print_dict(fail_2_dict)
#| message: false
# fits a pipe for successful games
trunc_succ_pipe_test = Pipeline([
("preprocessor", preprocessor),
("trunc", TruncatedSVD(10))
]).fit(succ)
# fits a pipe for unsuccessful games
trunc_fail_pipe_test = Pipeline([
("preprocessor", preprocessor),
("trunc", TruncatedSVD(10))
]).fit(fail)
#| message: false
# making scree plot
fig, ax = plt.subplots()
sns.lineplot(
x = np.arange(1, 11),
y = trunc_succ_pipe_test["trunc"].explained_variance_ratio_,
ax = ax)
ax.set(
xlabel = "Components",
ylabel = "PVE",
title = "Successful Games Scree Plot"
)
ax.xaxis.set_major_locator(MultipleLocator(1))
plt.tight_layout()
plt.show()
# making scree plot
fig, ax = plt.subplots()
sns.lineplot(
x = np.arange(1, 11),
y = trunc_fail_pipe_test["trunc"].explained_variance_ratio_,
ax = ax)
ax.set(
xlabel = "Components",
ylabel = "PVE",
title = "Unsuccessful Games Scree Plot"
)
ax.xaxis.set_major_locator(MultipleLocator(1))
plt.tight_layout()
plt.show()
# fits a truncatedsvd for successful games with optimal value
trunc_succ_pipe = Pipeline([
("preprocessor", preprocessor),
("trunc", TruncatedSVD(5))
]).fit(succ)
# fits a truncatedsvd for unsuccessful games  with optimal value
trunc_fail_pipe = Pipeline([
("preprocessor", preprocessor),
("trunc", TruncatedSVD(6))
]).fit(fail)
import random
# number of data to generate
n = 5001
# total number of games in our dataset
size = len(games)
# extracts publisher developer pairs
pub_devel = games[["PUBLISHER", "DEVELOPER"]]
# corresponds each unique value price to float between 0-1 (ratio)
prices = games["BASE PRICE"]
counts = prices.value_counts().reset_index()
counts.columns = ["base_price", "ratio"]
counts.ratio = counts.ratio/size
counts.ratio = counts.ratio.cumsum()
# length of counts
size_splits = len(counts)
# different playability options
playability = ["LAN Co-op" ,"Online Co-op", "Online PvP", "Shared/Split Screen Co-op", "Shared/Split Screen PvP", "Single-player"]
play_size = len(playability)
genres = ['Action', 'Adventure', 'Casual', 'Early Access', 'Indie', 'Massively Multiplayer', 'Movie', 'RPG', 'Racing', 'Simulation','Sports', 'Strategy']
genre_size = len(genres)
# intialized synthetic data's dataframe columns
synthetic = pd.DataFrame(columns=['DEVELOPER', 'PUBLISHER', 'BASE PRICE', 'Action',
'Adventure', 'Casual', 'Early Access', 'Free to Play', 'Indie',
'Massively Multiplayer', 'Movie', 'RPG', 'Racing', 'Simulation',
'Sports', 'Strategy', 'LAN Co-op', 'Online Co-op', 'Online PvP',
'Shared/Split Screen Co-op', 'Shared/Split Screen PvP',
'Single-player'])
for i in range(1,n+1):
# chooses a random pair of publisher and developer
random_row = random.randint(1, size)
company = pub_devel.iloc[random_row]
# generates a float from 0-1
random_price_split = random.random()
# gets price from random number
for j in range(0,size_splits):
if(random_price_split <= counts.iloc[j]["ratio"]):
price = counts.iloc[j]["base_price"]
break
random_play = random.randint(1, play_size)
play_s = random.sample(playability, random_play)
random_genre = random.randint(1, genre_size)
genre_s = random.sample(genres, random_genre)
# Create a dictionary representing the row
r1 = {'DEVELOPER': company['DEVELOPER'],
'PUBLISHER': company['PUBLISHER'],
'BASE PRICE': price}
r2 = {genre: 1 if genre in genre_s else 0 for genre in genres}
r3 = {play: 1 if play in play_s else 0 for play in playability}
new_row = {**r1,**r2,**r3}
synthetic = synthetic._append(new_row, ignore_index = True)
synthetic["Free to Play"] = 0
synthetic.loc[synthetic["BASE PRICE"] == 0, "Free to Play"] = 1
synthetic.to_csv('../data/synthetic_data.csv')
X = data.drop(["SUCCESS" ,"NAME","DEVELOPER", "PUBLISHER"], axis=1)
y = data["SUCCESS"]
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.9, random_state = 8264)
# categorical_columns = ["DEVELOPER", "PUBLISHER"]
# # define numerical columns
numerical_columns = ["BASE PRICE", "ALL TIME PEAK", "TOTAL REVIEWS","NEGATIVE REVIEWS"]
#
preprocessor = make_column_transformer(
(StandardScaler(), numerical_columns),
remainder='passthrough',
verbose_feature_names_out = False
)
pipeline_log = Pipeline(
[
("preprocessor", preprocessor),
("estimator", LogisticRegression())
]
)
pipeline_log.fit(X_train, y_train)
y_pred_log_data = pipeline_log.predict_proba(X_test)[:, 1]
log_auc_logs = roc_auc_score(y_test, y_pred_log_data)
rf_pipeline = Pipeline(
[
('preprocessor', preprocessor),
('rf', RandomForestRegressor(random_state=0))
]
)
b = np.arange(300, 501, 50)
m = np.arange(1, 6)
# Define the cross-validation strategy
cv = KFold(n_splits=5)
# Create a pipeline for Random Forest regression
# Define the grid of hyperparameters to search over
param_grid_rf = dict(
rf__n_estimators = b,
rf__max_features = m
)
# Perform grid search using cross-validation to find the best hyperparameters
grid_rf = (
GridSearchCV(
rf_pipeline,
param_grid = param_grid_rf,
cv = cv,
scoring = 'neg_mean_squared_error')
.fit(X_train, y_train)
)
# find predictions
y_pred_rf = grid_rf.predict_proba(X_test)[:,1]
# Evaluate the performance of the model
log_auc_rf = roc_auc_score(y_test, y_pred_rf)
library(readr)
library(dplyr)
library(tidyverse)
library(reticulate)
library(knitr)
reticulate::repl_python()
library(readr)
library(dplyr)
library(tidyverse)
library(reticulate)
library(knitr)
reticulate::repl_python()
